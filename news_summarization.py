# -*- coding: utf-8 -*-
"""News_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGdeQ5QLYF4fiCc6vKBKZDJzRjc1m97_
"""

!pip install transformers datasets

from datasets import load_dataset

# Load CNN/DailyMail dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")

# Display dataset structure
print(dataset)

# Access train, validation, and test splits
train_data = dataset["train"]
val_data = dataset["validation"]
test_data = dataset["test"]

# Print a sample from the dataset
print("Sample Article:", train_data[0]["article"])
print("Sample Summary:", train_data[0]["highlights"])

!huggingface-cli login

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-base")

# Test tokenizer on a sample article
sample_text = train_data[0]["article"]
inputs = tokenizer("summarize: " + sample_text, return_tensors="pt", max_length=512, truncation=True)
print("Tokenized Input IDs:", inputs["input_ids"])

# Generate a summary from the tokenized input
outputs = model.generate(inputs["input_ids"], max_length=128, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Generated Summary:", summary)

# Shuffle and sample 10,000 rows for training and validation
train_data = dataset["train"].shuffle(seed=42).select(range(10000))
val_data = dataset["validation"].shuffle(seed=42).select(range(10000))

# Define a preprocessing function
def preprocess_data(batch):
    # Add the "summarize:" prefix to input text to specify the summarization task
    inputs = ["summarize: " + article for article in batch["article"]]

    # Tokenize the input articles
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    # Tokenize the target summaries (labels)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["highlights"], max_length=128, truncation=True, padding="max_length")

    # Add tokenized labels to the model inputs
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

# Apply preprocessing to the dataset
train_data = train_data.map(preprocess_data, batched=True, remove_columns=["article", "highlights", "id"])
val_data = val_data.map(preprocess_data, batched=True, remove_columns=["article", "highlights", "id"])

# Display a sample from the preprocessed dataset
print(train_data[0])

from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",              # Directory to save the model checkpoints
    evaluation_strategy="epoch",        # Evaluate after each epoch
    learning_rate=5e-5,                 # Learning rate
    per_device_train_batch_size=4,      # Reduced batch size for larger model
    per_device_eval_batch_size=4,       # Reduced batch size for evaluation
    num_train_epochs=3,                 # Number of training epochs
    weight_decay=0.01,                  # Weight decay for regularization
    save_total_limit=2,                 # Limit the number of saved checkpoints
    logging_dir="./logs",               # Directory to save logs
    logging_steps=500,                  # Log every 500 steps
    save_strategy="epoch",              # Save the model at the end of every epoch
    load_best_model_at_end=True,        # Load the best model based on evaluation metric
    metric_for_best_model="eval_loss",  # Metric to determine the best model
    report_to="none"                    # Disable wandb logging
)

# Define a Trainer instance
trainer = Trainer(
    model=model,                          # The T5-base model to train
    args=training_args,                   # Training arguments
    train_dataset=train_data,             # Training dataset
    eval_dataset=val_data,                # Validation dataset
    tokenizer=tokenizer,                  # Tokenizer for input processing
)

# Train the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("./t5-base-finetuned")  # Update to save the T5-base model
tokenizer.save_pretrained("./t5-base-finetuned")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the fine-tuned T5-base model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained("./t5-base-finetuned")
tokenizer = AutoTokenizer.from_pretrained("./t5-base-finetuned")

# Set the model to evaluation mode
model.eval()

# Define a function to generate summaries
def generate_summary(article, max_input_length=512, max_output_length=128):
    """
    Generate a summary for a given article.

    Args:
        article (str): The input text (news article).
        max_input_length (int): Maximum token length for the input text.
        max_output_length (int): Maximum token length for the output summary.

    Returns:
        str: The generated summary.
    """
    # Tokenize the input article
    inputs = tokenizer(
        "summarize: " + article,
        return_tensors="pt",
        max_length=max_input_length,
        truncation=True
    )

    # Generate the summary
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_output_length,
        min_length=30,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )

    # Decode the output tokens
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

# Test the function with a sample article from the test dataset
test_article = dataset["test"][0]["article"]
true_summary = dataset["test"][0]["highlights"]

# Generate the summary
generated_summary = generate_summary(test_article)

# Print the original article, true summary, and generated summary
print("Original Article:")
print(test_article)
print("\nTrue Summary:")
print(true_summary)
print("\nGenerated Summary:")
print(generated_summary)

!pip install evaluate

!pip install rouge_score

from evaluate import load

# Load ROUGE metric
rouge = load("rouge")

# Generate summaries for the test dataset
def evaluate_model(dataset, max_samples=100):
    predictions = []
    references = []
    for i in range(min(max_samples, len(dataset))):
        article = dataset[i]["article"]
        reference = dataset[i]["highlights"]
        prediction = generate_summary(article)
        predictions.append(prediction)
        references.append(reference)
    return predictions, references

# Get predictions and references
predictions, references = evaluate_model(dataset["test"])

# Compute ROUGE scores
rouge_score = rouge.compute(predictions=predictions, references=references)
print("ROUGE Scores:", rouge_score)

"""The T5-base model demonstrates the ability to generate concise and fluent summaries. The generated summary successfully captures important details, such as the Palestinian Authority becoming the 123rd member of the International Criminal Court and the initiation of a preliminary examination in Palestinian territories.

The ROUGE scores for the model are as follows: ROUGE-1 at 0.33, ROUGE-2 at 0.137, ROUGE-L at 0.254, and ROUGE-Lsum at 0.282. These scores indicate a moderate level of overlap and structural alignment with the reference summaries.

While the model performs well in producing readable and coherent summaries, it occasionally misses critical information, such as details about opposition from the United States and Israel or the broader implications of the preliminary examination. Further adjustments in the future may improve its ability to encapsulate all essential points.
"""

model.save_pretrained("./t5-base-finetuned")
tokenizer.save_pretrained("./t5-base-finetuned")

import matplotlib.pyplot as plt
import numpy as np

# Data for length comparison
original_length = len(test_article.split())
generated_length = len(generated_summary.split())
true_summary_length = len(true_summary.split())

# Plot: Comparison of lengths
fig, ax = plt.subplots(figsize=(8, 6))
labels = ["Original Article", "Generated Summary", "True Summary"]
lengths = [original_length, generated_length, true_summary_length]

bars = ax.bar(labels, lengths, color=["#A8DADC", "#F4A261", "#E9C46A"], edgecolor="black", linewidth=1.2)

# Add value annotations on the bars
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height, f"{height}", ha='center', va='bottom', fontsize=12)

# Add titles and labels
ax.set_title("Length Comparison", fontsize=16, weight="bold", pad=20)
ax.set_ylabel("Number of Words", fontsize=14)
ax.set_xlabel("Text Type", fontsize=14)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.savefig("length_comparison.png")  # Save for Streamlit
plt.show()

# Data for ROUGE scores
rouge_scores = {
    "ROUGE-1": 0.3302900943667364,
    "ROUGE-2": 0.13689271759556568,
    "ROUGE-L": 0.2541750153094671,
    "ROUGE-Lsum": 0.28234117467808395
}

fig, ax = plt.subplots(figsize=(8, 6))
labels = list(rouge_scores.keys())
scores = list(rouge_scores.values())

bars = ax.bar(labels, scores, color=["#A8DADC", "#457B9D", "#F4A261", "#E76F51"], edgecolor="black", linewidth=1.2)

# Add value annotations on the bars
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height, f"{height:.2f}", ha='center', va='bottom', fontsize=12)

# Add titles and labels
ax.set_title("ROUGE Scores", fontsize=16, weight="bold", pad=20)
ax.set_ylabel("Score", fontsize=14)
ax.set_xlabel("ROUGE Metric", fontsize=14)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(axis='y', linestyle='--', alpha=0.7)
ax.set_ylim(0, 0.4)  # Adjust the y-axis limit for better visualization

plt.tight_layout()
plt.savefig("rouge_scores.png")  # Save for Streamlit
plt.show()